<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Lev Udaltsov</title>
    <description>Welcome to my blog!</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 22 Feb 2021 16:34:23 +0000</pubDate>
    <lastBuildDate>Mon, 22 Feb 2021 16:34:23 +0000</lastBuildDate>
    <generator>Jekyll v3.9.0</generator>
    
      <item>
        <title>Recipe Tracker 2021</title>
        <description>&lt;p&gt;As someone who enjoys cooking and does it every day, I’m always on the lookout for new recipes or foods to try.
Documenting this passion for food is something I’ve wanted to do for a long time but never knew what format would be best for it, 
I have no expertise in food blogging or food photography and have no interest in sharing my amateur photos on Social Media.
However, I think a single, continuously updated blog post that is dedicated to documenting some interesting food
experiments and experiences is something I’d like to maintain.
It will give me something regular to update this site with and also be a great reference for my future self to keep track of
recipes I’d otherwise forget about or lose track of.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Here is a catalogue of interesting food bits I’ve tried in 2021:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#focaccia-from-scratch&quot;&gt;Focaccia From Scratch&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#lobster-with-prawn-toast&quot;&gt;Lobster with Prawn Toast&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#vegan-hot-chicken-roll&quot;&gt;Vegan Hot Chicken Roll&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;focaccia-from-scratch&quot;&gt;Focaccia From Scratch&lt;/h3&gt;

&lt;p&gt;I made this based on a recommendation from a friend and having seen the Claire Saffitz video on how to make one, and it turned out pretty well.
I used the BBC Good Food base recipe and experimented with the toppings based on Claire’s video, adding some Red Harissa Pesto on one side of the loaf.
It was really easy to make and turned out well, highly recommend giving it a go!&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Garlic, Rosemary and Chilli oil topping&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Close up of structure&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/images/food/focaccia1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/images/food/focaccia2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Links:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.bbcgoodfood.com/recipes/focaccia&quot;&gt;BBC Good Food Focaccia&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=NGnMrM9qDtE&quot;&gt;Claire Saffitz Recipe&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;lobster-with-prawn-toast&quot;&gt;Lobster with Prawn Toast&lt;/h3&gt;

&lt;p&gt;I made these two recipes as a Valentine’s day dine-in treat as all restaurants were closed.&lt;/p&gt;

&lt;p&gt;It was my second time ever cooking lobster but it was a lot easier than I thought it would be! I had to watch a few videos on Youtube
to ensure I didn’t mess up the lobster tails but it worked out in the end. I used 3 times as much garlic and butter than in the recipe,
which is quite conservative with its measurements. I served it with goose fat potatoes and some veg, and the excess melted garlic butter
made for a delicious sauce.&lt;/p&gt;

&lt;p&gt;Baked Lobster, roast potatoes, veg
&lt;img src=&quot;/assets/images/food/lobster.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The prawn toast I remembered from an old Tesco pre-made Chinese finger food range, but came across a recipe for it and thought I’d try it.
It was super simple to cook and really decadent, and made for a great little starter. Definitely going to make it for a special occasion in the future.&lt;/p&gt;

&lt;p&gt;Prawn Toast Cross-section
&lt;img src=&quot;/assets/images/food/prawntoast.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Links:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://tasty.co/recipe/baked-lobster-tails&quot;&gt;Lobster Written Recipe&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=BeLTD9lA34s&quot;&gt;Lobster Prep Video&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=i8fygRZlYHc&quot;&gt;Prawn Toast Recipe&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;vegan-hot-chicken-roll&quot;&gt;Vegan Hot Chicken Roll&lt;/h3&gt;

&lt;p&gt;As a proud Irish citizen, there are few things that bring me more comfort than a fresh,
crunchy deli Hot Chicken Roll (or Chicken Fillet Roll depending where you’re from).&lt;/p&gt;

&lt;p&gt;At the same time, I have recently been trying to eat fewer animal products while being able to enjoy a bit of indulgence now and then.
So when I came across &lt;strong&gt;Plant Kitchen no-chicken southern fried tenders&lt;/strong&gt; in Marks and Spencers I knew I had to stick it in a bread roll.
There isn’t much of a recipe here but I this was really tasty and had all the flavour of a standard Hot Chicken Roll so I thought I’d share!&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/images/food/hcr1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/images/food/HCR2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The taste was really similar to what you’d get in a deli HCR, although the texture wasn’t quite there.
Either way it was great to have a tasty plant-based twist on a classic.&lt;/p&gt;

</description>
        <pubDate>Thu, 18 Feb 2021 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/recipe-tracker/</link>
        <guid isPermaLink="true">http://localhost:4000/recipe-tracker/</guid>
        
        <category>Food</category>
        
        <category>Photography</category>
        
        
        <category>Food</category>
        
      </item>
    
      <item>
        <title>An Album paints a Thousand Words: NLP And Autoencoders for Music Recommendation</title>
        <description>&lt;p&gt;They say a picture paints a thousand words.
The same could be said for a piece of music, or any work of art for that matter.
While I don’t frequent art forums, I have in the past visited music review sites and have read many of the reviews
posted there, be they reviews of singles, albums, or concerts.
Often, a thousand words or more (usually less, as we’ll see) are used directly to describe a piece of music by a
journalist trying to critically analyse it. Perhaps an analysis of these words directly can give us an insight as to how we can
categorize the album involved.&lt;/p&gt;

&lt;p&gt;A few months ago, I stumbled upon &lt;a href=&quot;https://www.kaggle.com/nolanbconaway/pitchfork-data&quot;&gt;this&lt;/a&gt; dataset on 
Kaggle which contains a whopping 18,393 album reviews from the music website Pitchfork, containing all reviews
posted from Jan 5, 1999 to Jan 8, 2017.
I found this dataset at a time when I was performing research on Autoencoders for a project in work
and I was keen to apply this cool Machine Learning architecture
to an interesting real-world dataset. Using some common NLP (Natural Language Processing) techniques and a simple
Autoencoder Neural Network I was able to achieve just that. The idea I had was:&lt;/p&gt;

&lt;h3 id=&quot;would-it-be-possible-to-create-album-recommendations-calculating-the-distance-between-albums-in-some-latent-space-using-an-autoencoder&quot;&gt;Would it be possible to create album recommendations calculating the distance between albums in some latent space, using an autoencoder?&lt;/h3&gt;

&lt;p&gt;You may be asking;&lt;/p&gt;

&lt;h2 id=&quot;what-is-an-autoencoder&quot;&gt;What is an Autoencoder?&lt;/h2&gt;

&lt;p&gt;An &lt;strong&gt;Autoencoder&lt;/strong&gt; is a type of Neural Network that consists of an encoder and a decoder and is primarily used 
to perform dimensionality reduction on high-dimensional data. It has other uses, such as image compression, 
feature extraction, and more. The &lt;a href=&quot;https://iq.opengenus.org/applications-of-autoencoders/&quot;&gt;following&lt;/a&gt; is a nice
article running through the other uses, giving some nice visual examples.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://datakeen.co/wp-content/uploads/2018/02/S%C3%A9lection_106.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;A rough sketch of an autoencoder. Source: datakeen.co&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The main idea is that you feed data into the neural network, and train the model by comparing the
reconstructed data outputs to the original inputs. Using the above image an example, you reduce dimensionality of a sample
from layer &lt;em&gt;X&lt;/em&gt; to layer &lt;em&gt;Z&lt;/em&gt;, e.g 784-dimensional data to 2 dimensional data, using an encoder. You then decode the
2-dimensional data to the 784-dim data using the decoder and compare the output to the original input.
The difference between input and output is what the network is trained on.&lt;/p&gt;

&lt;p&gt;The thinking is that if your output &lt;em&gt;X’&lt;/em&gt; matches perfectly with original input &lt;em&gt;X&lt;/em&gt;, then the 2-dimensional data 
generated by the encoder part of the network is a good representation of of the 784-dimensional data.&lt;/p&gt;

&lt;p&gt;The lower dimensional data can then be used for other tasks such as clustering.&lt;/p&gt;

&lt;h2 id=&quot;analysing-text-using-tf-idf&quot;&gt;Analysing text using TF-IDF&lt;/h2&gt;

&lt;p&gt;To apply this modelling approach to the album review dataset, you would first have to turn the albums
into some dataset that can be fed into a neural network. To do this, I decided to use a statistical 
technique called &lt;strong&gt;TF-IDF&lt;/strong&gt;, or &lt;strong&gt;T&lt;/strong&gt;erm &lt;strong&gt;F&lt;/strong&gt;requency-&lt;strong&gt;I&lt;/strong&gt;nverse &lt;strong&gt;D&lt;/strong&gt;ocument &lt;strong&gt;F&lt;/strong&gt;requency. This is used to measure the
relevance of a word in a document, also taking into account how often it occurs in an entire set of documents.
This relevance is assigned a numeric score, and a selection of key words can be selected to describe
a set of documents.&lt;/p&gt;

&lt;p&gt;A brief explanation if this technique can be found &lt;a href=&quot;https://monkeylearn.com/blog/what-is-tf-idf/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here is a toy example. If we take all the albums in the dataset and apply TF-IDF to find the 10 most
relevant words to the dataset and how important they are for each document, we get a dataframe that looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/albums/album_df.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the example above, TF_IDF returned that the word &lt;em&gt;rock&lt;/em&gt; may be used to describe some albums, where they have a positive number in that column.
If the album has a 0 for the &lt;em&gt;rock&lt;/em&gt; column, it indicates it was not used, or not used enough to be considered relevant.
The larger the value; the stronger the relevance.
For Mezzanine, words &lt;em&gt;album&lt;/em&gt; and &lt;em&gt;band&lt;/em&gt; appear more important than &lt;em&gt;rock&lt;/em&gt; or &lt;em&gt;sound&lt;/em&gt;, due to the frequency of occurrence within the review.
This gives is a 10 dimensional vector to represent each document.
These vectors don’t contain a lot of information, and in practice we would like some informative data to feed to the model.&lt;/p&gt;

&lt;p&gt;I looked at the distribution of album length to decide how many of the words I want TF-IDF to identify from each document in the dataset.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/albums/albumhist.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There is an average of 650 words in each review, however there will always be words used in some reviews not used in others,
and I figured going for a round 1000 words would cover a lot of the text data.
I later experimented with this number, but adding more than 1000 words didn’t seem
to add much information, although a proper investigation into this would be interesting.&lt;/p&gt;

&lt;h2 id=&quot;applying-an-autoencoder-to-vectorized-dataset&quot;&gt;Applying an autoencoder to vectorized dataset&lt;/h2&gt;

&lt;p&gt;With those two explanations out of the way, we can look at the results of the modelling.
For the autoencoder implementation I used PyTorch, and for the TF-IDF I used the scikit-learn TfidfVectorizer.
All of the code I used can be found on my &lt;a href=&quot;https://github.com/LevUdaltsov/album_recommender/tree/main&quot;&gt;Github page&lt;/a&gt;.
The autoencoder consists of an encoder with an input layer of 1000, hidden layers 256, 64, 12, 2, and a decoder with hidden
layers 2, 12, 64, 256, and an output layer of 1000. I experimented with different amounts of layers and different layers, 
and found this combination to work well. It was trained for 200 epochs with a batch size of 100.&lt;/p&gt;

&lt;p&gt;The hyperparameter choice requires a lot more testing and tuning but for an initial experiment this was sufficient.&lt;/p&gt;

&lt;p&gt;So, what can we learn from encoding the album reviews? Here is a couple examples of artists and their albums plotted in the 
2-dimensional space to which the data was encoded.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/albums/album_plot_1.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;Plot of 2 dimensional representations of albums for four selected artists&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In the above plot we see that the model was able to learn some sort of information about the albums. For each artist,
the albums seem to cluster within a certain space, with a few outliers for the Kanye and Kendrick albums.
Coldplay’s albums are closest together. This is really interesting to see.&lt;/p&gt;

&lt;p&gt;Here is another example, this time without labels to get a clearer image.
&lt;img src=&quot;/assets/images/albums/album_plot_no_label.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Again, we see that aside from a few stragglers, albums are being plotted near each other.
In both plots, the Hip-Hop albums are plotted closer to other Hip-Hop albums, than the Rock or Pop or Electronic Music
records are.&lt;/p&gt;

&lt;p&gt;Using Autoencoders and NLP it was possible to boil down lengthy text reviews to data in 2 dimensional space!&lt;/p&gt;

&lt;h2 id=&quot;using-results-for-recommendation&quot;&gt;Using results for recommendation&lt;/h2&gt;

&lt;p&gt;Next, I tried using these points in space as a way of recommending new music. 
Using the geometric mean of a number of albums, I would located the closest album that had a pitchfork rating greater than 8.0,
and passed it as a recommendation.
&lt;img src=&quot;/assets/images/albums/recommendation_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here is a visualisation of these four albums, and the recommendation it returns located in between them.
&lt;img src=&quot;/assets/images/albums/recommendation.png&quot; alt=&quot;&quot; /&gt;.&lt;/p&gt;

&lt;p&gt;What the system returned was a an album of various old soul/funk songs from a small Detroit label;
&lt;a href=&quot;https://open.spotify.com/album/2dGg3NFqftr5ufcgzakbW0?si=D3TEZ70JTSKwPyNXV3kuCw&quot;&gt;&lt;em&gt;Eccentric  Soul: The Big Mack Label&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It’s a great listen and I’m listening to it now as I edit this article! The model generated a few other recommendations
that I have really enjoyed and would never have come across myself. I will post another article outlining these soon.&lt;/p&gt;

&lt;p&gt;Overall, while I haven’t figured out how to apply standard recommendation system testing or validation to this, 
I think that it has been a very interesting experiment. Some next steps I have in mind are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Comprehensibilty Analysis&lt;/li&gt;
  &lt;li&gt;Hyper Parameter fine-tuning&lt;/li&gt;
  &lt;li&gt;Application to other datasets&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Keep an eye out for these in future posts on my blog!&lt;/p&gt;
</description>
        <pubDate>Thu, 14 Jan 2021 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/album-autoencoder/</link>
        <guid isPermaLink="true">http://localhost:4000/album-autoencoder/</guid>
        
        <category>Deep Learning</category>
        
        <category>Music</category>
        
        <category>NLP</category>
        
        
        <category>Machine Learning</category>
        
      </item>
    
  </channel>
</rss>
